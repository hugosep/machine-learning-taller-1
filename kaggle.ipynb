{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hugo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hugo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'category'], dtype='object')\n",
      "Index(['Id', 'title', 'Predicted'], dtype='object')\n",
      "\n",
      "title       0\n",
      "category    0\n",
      "dtype: int64\n",
      "Id               0\n",
      "title            0\n",
      "Predicted    71461\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('ml-desafio/train.csv', sep=\",\")\n",
    "x_test = pd.read_csv('ml-desafio/x_test.csv', sep=\",\")\n",
    "\n",
    "print(train.columns)\n",
    "print(x_test.columns)\n",
    "print(\"\")\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(x_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [bikini, malla, lunares, colores, colección, v...\n",
      "1                          [kit, bujias, ngk, ford, bp5efs]\n",
      "2         [campera, rompeviento, adidas, original, hard,...\n",
      "3                     [vulk, danner, sblk, sg91, polarized]\n",
      "4                                 [calza, frizada, termica]\n",
      "                                ...                        \n",
      "107186    [memoria, ddr4, kingston, cl15, hyperx, fury, ...\n",
      "107187                [mallas, talles, grandes, importadas]\n",
      "107188        [kit, bujias, originales, bmw, e53, m62, b46]\n",
      "107189    [reloj, casio, unisex, retro, vintage, metal, ...\n",
      "107190    [reloj, montreal, mujer, ml369, tienda, oficia...\n",
      "Name: new_title, Length: 107191, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TfidfVectorizer().build_tokenizer()#Return a function that splits a string into a sequence of tokens considering unicode characters\n",
    "stemmer = SnowballStemmer(\"spanish\") \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "##stop-words: el vectorizador ya las remueve pero para mostrar las palabras más frecuente tiene sentido activarlo\n",
    "remove_stops_here = False\n",
    "\n",
    "def pre_processing(text):\n",
    "    results = []\n",
    "    for token in tokenizer(text):\n",
    "        clean_token = token.lower().strip().strip('-').strip('_')\n",
    "        if remove_stops_here and (clean_token in stopwords.words('spanish')):\n",
    "            continue\n",
    "        #token_pro = stemmer.stem(clean_token) #podemos probar stemming en vez de lematizacion\n",
    "        token_pro = lemmatizer.lemmatize(clean_token) \n",
    "        if len(token_pro) > 2 and not token_pro[0].isdigit(): #elimina palabra largo menor a 2\n",
    "            results.append(token_pro)\n",
    "    return results\n",
    "\n",
    "train[\"new_title\"] = train[\"title\"].apply(pre_processing)\n",
    "print(train[\"new_title\"])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(train[\"title\"], train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107191, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "max_features = 10000\n",
    "max_df=0.8\n",
    "min_df=2\n",
    "\n",
    "### NUMBER OF N GRAMS TO CONSIDER\n",
    "ngram_max = 1\n",
    "\n",
    "#vectorizer = TfidfVectorizer(stop_words=spanish_stopwords,tokenizer=pre_processing,min_df=min_df, max_df=max_df, max_features=max_features,\n",
    " #                             binary=False, use_idf=True, smooth_idf=True, norm=None, ngram_range=(1, ngram_max))\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=spanish_stopwords,tokenizer=pre_processing,min_df=min_df, max_df=max_df, max_features=max_features,\n",
    "                              binary=True, ngram_range=(1, ngram_max))\n",
    "#CountVectorizer\n",
    "#TfidfVectorizer\n",
    "\n",
    "vectorizer.fit(train['title'])\n",
    "x_train_binary = vectorizer.transform(train['title'])\n",
    "\n",
    "print(x_train_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9877974829976397\n",
      "['NECKLACES' 'MICROPHONES' 'SMARTWATCHES' ... 'COMPUTER_PROCESSORS'\n",
      " 'MOTORCYCLE_HELMETS' 'SPARK_PLUGS']\n",
      "0            0\n",
      "1            1\n",
      "2            2\n",
      "3            3\n",
      "4            4\n",
      "         ...  \n",
      "71456    71456\n",
      "71457    71457\n",
      "71458    71458\n",
      "71459    71459\n",
      "71460    71460\n",
      "Name: Id, Length: 71461, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = x_train_binary\n",
    "Y = train[\"category\"]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, Y)\n",
    "\n",
    "X_test = vectorizer.transform(x_test['title'])\n",
    "print(lr.score(X, Y))\n",
    "Y_predicted = lr.predict(X_test)\n",
    "\n",
    "print(Y_predicted)\n",
    "print(x_test[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            0\n",
      "1            1\n",
      "2            2\n",
      "3            3\n",
      "4            4\n",
      "         ...  \n",
      "71456    71456\n",
      "71457    71457\n",
      "71458    71458\n",
      "71459    71459\n",
      "71460    71460\n",
      "Name: Id, Length: 71461, dtype: int64 <class 'pandas.core.frame.DataFrame'>\n",
      "                 Predicted\n",
      "0                NECKLACES\n",
      "1              MICROPHONES\n",
      "2             SMARTWATCHES\n",
      "3       MOTORCYCLE_HELMETS\n",
      "4                    PANTS\n",
      "...                    ...\n",
      "71456     DIECAST_VEHICLES\n",
      "71457   RAM_MEMORY_MODULES\n",
      "71458  COMPUTER_PROCESSORS\n",
      "71459   MOTORCYCLE_HELMETS\n",
      "71460          SPARK_PLUGS\n",
      "\n",
      "[71461 rows x 1 columns]\n",
      "          Id            Predicted\n",
      "0          0            NECKLACES\n",
      "1          1          MICROPHONES\n",
      "2          2         SMARTWATCHES\n",
      "3          3   MOTORCYCLE_HELMETS\n",
      "4          4                PANTS\n",
      "...      ...                  ...\n",
      "71456  71456     DIECAST_VEHICLES\n",
      "71457  71457   RAM_MEMORY_MODULES\n",
      "71458  71458  COMPUTER_PROCESSORS\n",
      "71459  71459   MOTORCYCLE_HELMETS\n",
      "71460  71460          SPARK_PLUGS\n",
      "\n",
      "[71461 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[\"Id\"], type(x_test))\n",
    "y = pd.DataFrame(data=Y_predicted, columns=[\"Predicted\"])\n",
    "print(y)\n",
    "y_predicted = pd.concat([x_test[\"Id\"], y], axis=1)\n",
    "print(y_predicted)\n",
    "y_predicted.to_csv(\"sample_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
